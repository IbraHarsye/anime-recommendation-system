# -*- coding: utf-8 -*-
"""Anime Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sogA-L4OMWsp1OhYqaiUlf2taOvZJCkP

# Recommender system for Anime

# Import Library

Melakukan import library yang digunakan untuk melakukan manipulasi data dan modelling content-based filtering dan collaborative filtering dalam membuat sistem rekomendasi
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import  EarlyStopping

"""# Data Loading

Melakukan loading dataset untuk membuat sistem rekomendasi dari kaggle. dataset dapat diakses melalui [di sini](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database)
"""

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download CooperUnion/anime-recommendations-database

!unzip /content/anime-recommendations-database.zip -d /content/

anime = pd.read_csv('content/anime.csv')
print('Jumlah data judul anime: ', len(anime.anime_id.unique()))
anime

rating = pd.read_csv('content/rating.csv')
print('Jumlah data rating: ', len(rating.user_id.unique()))
rating

"""# Data Understanding

## File anime.csv
"""

anime.info()

print('Banyak Judul Anime: ', len(anime.anime_id.unique()))
print('Genre Anime: ', anime.genre.unique())

"""Terdapat 12294 judul anime yang terdapat pada dataset"""

anime.describe()

"""Rata-rata rating anime yang diberikan user berkisar antara 1.67 sampai 10

## File rating.csv
"""

rating.info()

print('Banyak user yang memberikan rating: ', len(rating.user_id.unique()))
print('Id Anime: ', rating.anime_id.unique())

print(rating.shape)

rating.describe()

"""Terdapat noise dalam data pada kolom rating, dimana terdapat rating dari user bernilai -1. data ini nanti akan dihapus"""

print('Jumlah anime yang dirating: ', len(rating.anime_id.unique()))
print('Jumlah data rating: ', len(rating))

"""Ternyata tidak semua anime yang berada pada database anime diberikan rating oleh user"""

# Visualisasi untuk kolom type pada dataset anime
plt.figure(figsize=(8,4))
sns.countplot(x=anime["type"])

"""Pada visualisasi diatas, type anime terbanyak adalah TV"""

# Visualisasi untuk rating oleh user
# rating_viz = rating["rating"].replace({-1: np.nan}, inplace=True)
# rating_viz = rating.dropna(axis = 0, how ='any')
plt.figure(figsize=(8,4))
sns.countplot(x=rating_viz["rating"])

"""Rating terbanyak yang diberikan adalah 8

# Data Preparation

Selanjutnya menggabungkan anime dan rating dengan menggunakan anime_id

### Content-based filtering
"""

anime_all = pd.merge(anime, rating, on='anime_id', suffixes=['', '_user'])
anime_all

"""Mengisi kolom genre, type, dan rating yang null agar informasi penting lainnya tidak hilang"""

ratingsMean = anime.rating.mean()
anime_all["rating_user"].replace({-1: np.nan}, inplace=True)
anime_all['genre'] = anime_all['genre'].fillna('TBD')
anime_all['type'] = anime_all['type'].fillna('TBD')
anime_all['rating'] = anime_all['rating'].fillna(ratingsMean)

anime_all.isnull().sum()

"""Menghapus rating user yang bernilai -1"""

anime_all = anime_all.dropna(axis = 0, how ='any')
anime_all.isnull().sum()

"""## Collaborative Filtering

Memilih data anime dengan penggemar lebih dari 250000 agar pengguna direkomendasikan anime yang cukup terkenal
"""

anime250k = anime_all[anime_all.members > 250000]
anime250k

anime_fix = anime250k
anime_fix.sort_values('members')

"""Selanjutnya akan dilakukan konversi data dari data series menjadi list dan akan dilakukan encode pada kolom user_id dan anime_id dan akan dilakukan mapping terhadap dua kolom tersebut"""

# Mengubah user_id menjadi list
user_id = anime_fix['user_id'].unique().tolist()
print('list user_id: ', user_id)

# Encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
print('encoded user_id : ', user_to_user_encoded)

# Encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_id)}

print('encoded angka ke user_id: ', user_encoded_to_user)

# Mengubah anime_id menjadi list
anime_id = anime_fix['anime_id'].unique().tolist()
print('list anime_id: ', anime_id)

# Encoding anime_id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_id)}
print('encoded anime_id : ', anime_to_anime_encoded)

# Encoding angka ke anime_id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_id)}

print('encoded angka ke anime_id: ', anime_encoded_to_anime)

"""# Modelling

Untuk sistem rekomendasi content based filtering akan digunaka cosine similarity untuk menentukan kemiripan antar item yang nantinya bisa direkomendasikan kepada user yang juga melihat item tersebut.

Untuk sistem rekomendasi collaborative filtering akan digunakan deep learning yang akan melakukan train data dan menampilkan rekomendasi untuk satu user.

## Content based Filtering

Untuk content based filtering akan digunakan dataset pada file anime.csv
"""

anime.isnull().sum()

"""kita hanya akan menggunakan kolom anime_id, name, dan genre"""

anime.duplicated().sum()

"""Tidak terdapat data duplikat pada dataset

Mengisi kolom genre pada anime yang bernilai NaN
"""

anime['genre'] = anime['genre'].fillna('')
genres_str = anime['genre'].str.split(',').astype(str)

anime.isnull().sum()

tfv = TfidfVectorizer(min_df=3,  max_features=None,
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3),
            stop_words = 'english')

tfv_matrix = tfv.fit_transform(genres_str)

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfv_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime['name'],
                             columns=anime['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix tiap anime
cosine_sim_df.sample(10, axis=1).sample(10, axis=0)

def anime_recommendations(anime_title, similarity_data=cosine_sim_df,
                         items=anime[['name','genre','type','rating']], k=10):


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)

    index = similarity_data.loc[:, anime_title].to_numpy().argpartition(
        range(-1, -k, -1)
    )

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop anime_title agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(anime_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

anime[anime.name.eq('Boruto: Naruto the Movie - Naruto ga Hokage ni Natta Hi')]

anime_name = input('What anime do you like ?')
df2 = cosine_sim_df.filter(regex=anime_name)
column_headers = list(df2.columns.values)
for i in column_headers :
  print(i)
choosen_anime = input("Copy and paste the title here :")
print ('You"ll like this anime :')
anime_recommendations(choosen_anime)

"""Model berhasil memberikan rekomendasi 10 judul Anime dengan Genre yang sama seperti yang disukai, yaitu Action, Comedy, Martial Arts, Shounen, dan Super Power.

## Collaborative Filtering
"""

anime_cf = anime_fix
anime_cf

anime_cf.duplicated().sum()

anime_cf = anime_cf.drop_duplicates()

# Mapping user_id ke dataframe user
anime_cf['user'] = anime_cf['user_id'].map(user_to_user_encoded)

# Mapping anime_id ke dataframe anime
anime_cf['anime'] = anime_cf['anime_id'].map(anime_to_anime_encoded)

"""Melakukan cek jumlah user, anime, dan mengubah tipe data rating menjadi float"""

# Mendapatkan jumlah user
num_user = len(user_to_user_encoded)

# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)

# Mengubah rating menjadi nilai float
anime_cf['rating_user'] = anime_cf['rating_user'].values.astype(np.float32)
anime_cf['anime'] = anime_cf['anime'].values.astype(np.int64)
# Nilai minimum rating
min_rating = min(anime_cf['rating_user'])

# Nilai maksimal rating
max_rating = max(anime_cf['rating_user'])

print('Jumlah User: {}, Jumlah anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_user, num_anime, min_rating, max_rating
))

"""Selanjutnya dilakukan shuffle data agar distribusinya menjadi random"""

anime_cf = anime_cf.sample(frac=1, random_state=42)
anime_cf

"""Melakukan inisialisasi untuk variabel x dan variabel y dan membagi data dengan proporsi 80%:20%"""

x = anime_cf[['user', 'anime']].values


y = anime_cf['rating_user'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values


train_indices = int(0.9 * anime_cf.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Membuat model deep learning untuk melakukan training data sistem rekomendasi collaborative filtering"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x) # activation sigmoid

# inisialisasi model RecommenderNet
model = RecommenderNet(num_user, num_anime, 50)

# Compile Model RecommenderNet
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[[tf.keras.metrics.RootMeanSquaredError()]]
)

"""Model RecommenderNet menggunakan binary crossentropy untuk meminimalkan loss, Adam sebagai fungsi optimisasinya dan menggunakan mean absolute error (MAE) dan Root Mean Squared Error (RMSE) sebagai metrik evaluasinya."""

#Melakukan inisialisasi callbacks untuk model
callbacks = EarlyStopping(
    monitor ='val_root_mean_squared_error' ,
    mode='min',
    patience=1,
    restore_best_weights=True,
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[callbacks]
)

# Mengambil sample user
user_id = anime_cf.user_id.sample(1).iloc[0]
anime_watched_by_user = anime_cf[anime_cf.user_id == user_id]

# Operator bitwise
anime_not_watched = anime_cf[~anime_cf['anime_id'].isin(anime_watched_by_user.anime_id.values)]['anime_id']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_watched = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

ratings = model.predict(user_anime_array).flatten()

# top rating
top_ratings_indices = ratings.argsort()[-10:][::-1]

# rekomendasi anime
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print('Menampilkan rekomendasi untuk user: {}'.format(user_id))
print('=' * 9)
print('anime dengan peringkat tinggi dari user')
print('-' * 8)

# mencari rekomendasi anime berdasarkan rating yang diberikan user
top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating_user',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

df_anime_rows = anime[anime['anime_id'].isin(top_anime_user)]
for row in df_anime_rows.itertuples():
    print(row.name, ':', row.genre)

print('-' * 8)
print('10 rekomendasi anime teratas')
print('-' * 8)

# rekomendasi anime
anime_top10 = anime[anime['anime_id'].isin(recommended_anime_ids)]

# fungsi perulangan untuk menampilkan rekomendasi anime dan genre sebanyak 10 buah
for row in anime_top10.itertuples():
    print(row.name, ':', row.genre)

"""# Model Evaluation"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['root_mean_squared_error', 'val_root_mean_squared_error'], loc='upper left')
plt.show()